# ğŸ›°ï¸ Satellite Disaster Response

## ğŸ“˜ Overview
This project leverages deep learning and satellite imagery to assess disaster damage automatically.
By analyzing pre- and post-disaster satellite images, the model classifies affected areas into multiple
damage levels, enabling faster and more accurate disaster response and recovery operations.

---

## âš™ï¸ Problem Statement
During large-scale disasters such as earthquakes, floods, or hurricanes, rapid assessment of infrastructure
damage is crucial. Manual inspection is time-consuming and inconsistent.
This project automates the process using computer vision models trained on satellite imagery.

---

## ğŸ¯ Motivation & Impact
- âš¡ Accelerate post-disaster analysis and response time.
- ğŸ›°ï¸ Utilize open satellite data for real-world humanitarian applications.
- ğŸ¤– Reduce human dependency and subjective bias in damage assessment.
- ğŸŒ Support rescue planning and prioritization using AI predictions.
- ğŸ’¡ Demonstrate how machine learning can contribute to social good.

---

## ğŸ§© Project Workflow
1. **Data Preprocessing**
   - Dataset used: `xView2` Disaster Dataset.
   - Pre- and Post-disaster images were resized, normalized, and augmented.
   - Data split into training, validation, and testing sets.

2. **Model Training**
   - Baseline: `ResNet18` (transfer learning from ImageNet).
   - Transformer-based models:
     - `Vision Transformer (ViT)`
     - `Swin Transformer`
   - Loss function: CrossEntropyLoss
   - Optimizer: Adam / AdamW
   - Learning Rate Scheduling used for optimization stability.

3. **Evaluation**
   - Accuracy and F1-score used as performance metrics.
   - Confusion matrix and per-class performance visualized.
   - Validation accuracy reached ~78.7% (best model).

4. **Visualization**
   - Sample predictions visualized using `matplotlib`.
   - Grad-CAM visualizations for understanding damaged regions.

---

## ğŸ§  Model Architectures
- **ResNet18**: Convolutional backbone for strong baseline performance.
- **Vision Transformer (ViT)**: Captures global context using self-attention.
- **Swin Transformer**: Hierarchical representation learning for fine-grained spatial understanding.

---

## ğŸ“Š Results
| Model | Validation Accuracy | Notes |
|--------|--------------------|-------|
| ResNet18 | 78.76% | Strong baseline CNN |

---

## ğŸ§ª Tools & Libraries
```python
torch          # Deep learning framework
torchvision    # Pretrained models and transforms
matplotlib     # Visualization
numpy          # Numerical processing
PIL            # Image processing
scikit-learn   # Metrics and evaluation
```

---

## How to Run

# 1ï¸âƒ£ Clone the repository
git clone https://github.com/<your-username>/Satellite-Disaster-Response.git
cd Satellite-Disaster-Response

# 2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

# 3ï¸âƒ£ Run the notebook
jupyter notebook Satellite_Disater.ipynb

# 4ï¸âƒ£ (Optional) Visualize results
python visualize_predictions.py
